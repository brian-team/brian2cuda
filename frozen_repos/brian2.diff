diff --git a/brian2/conftest.py b/brian2/conftest.py
index 5a97ac81..c36a40a7 100644
--- a/brian2/conftest.py
+++ b/brian2/conftest.py
@@ -6,7 +6,7 @@
 import numpy as np
 import pytest
 
-from brian2.devices import reinit_devices
+from brian2.devices import reinit_devices, get_device
 from brian2.units import ms
 from brian2.core.clocks import defaultclock
 from brian2.core.functions import Function, DEFAULT_FUNCTIONS
@@ -99,6 +99,9 @@ def pytest_runtest_makereport(item, call):
     outcome = yield
     rep = outcome.get_result()
     if rep.outcome == 'failed':
+        project_dir = get_device().project_dir
+        if project_dir is not None:
+            rep.sections.append(('Standalone project directory', f'{project_dir}'))
         reinit_devices()
         if not fail_for_not_implemented:
             exc_cause = getattr(call.excinfo.value, '__cause__', None)
diff --git a/brian2/core/preferences.py b/brian2/core/preferences.py
index 3663c276..40cc65a6 100644
--- a/brian2/core/preferences.py
+++ b/brian2/core/preferences.py
@@ -6,7 +6,7 @@
 import re
 import os
 from collections.abc import MutableMapping
-from io import BytesIO
+from io import StringIO
 
 from brian2.utils.stringtools import deindent, indent
 from brian2.units.fundamentalunits import have_same_dimensions, Quantity
@@ -457,7 +457,7 @@ def reset_to_defaults(self):
         '''
         Resets the parameters to their default values.
         '''
-        self.read_preference_file(BytesIO(self.defaults_as_file))
+        self.read_preference_file(StringIO(self.defaults_as_file))
 
     def register_preferences(self, prefbasename, prefbasedoc, **prefs):
         '''
diff --git a/brian2/devices/cpp_standalone/device.py b/brian2/devices/cpp_standalone/device.py
index 5791bb61..57fe3100 100644
--- a/brian2/devices/cpp_standalone/device.py
+++ b/brian2/devices/cpp_standalone/device.py
@@ -65,6 +65,21 @@
         neurons. Now, its value is ignored.
         '''
         ),
+    make_cmd_unix=BrianPreference(
+        default='make',
+        docs='''
+        The make command used to compile the standalone project. Defaults to the
+        standard GNU make commane "make".'''
+        ),
+    run_cmd_unix=BrianPreference(
+        default='./main',
+        validator=lambda val: isinstance(val, str) or isinstance(val, list),
+        docs='''
+        The command used to run the compiled standalone project. Defaults to executing
+        the compiled binary with "./main". Must be a single binary as string or a list
+        of command arguments (e.g. ["./binary", "--key", "value"]).
+        '''
+        ),
     extra_make_args_unix=BrianPreference(
         default=['-j'],
         docs='''
@@ -978,8 +993,9 @@ def compile_source(self, directory, compiler, debug, clean):
                 with std_silent(debug):
                     if clean:
                         os.system('make clean >/dev/null 2>&1')
+                    make_cmd = prefs.devices.cpp_standalone.make_cmd_unix
                     make_args = ' '.join(prefs.devices.cpp_standalone.extra_make_args_unix)
-                    x = os.system('make %s' % (make_args, ))
+                    x = os.system('%s %s' % (make_cmd, make_args, ))
                     if x != 0:
                         error_message = ('Project compilation failed (error '
                                          'code: %u).') % x
@@ -1019,7 +1035,10 @@ def run(self, directory, with_output, run_args):
             if os.name == 'nt':
                 x = subprocess.call(['main'] + run_args, stdout=stdout)
             else:
-                x = subprocess.call(['./main'] + run_args, stdout=stdout)
+                run_cmd = prefs.devices.cpp_standalone.run_cmd_unix
+                if isinstance(run_cmd, str):
+                    run_cmd = [run_cmd]
+                x = subprocess.call(run_cmd + run_args, stdout=stdout)
             if stdout is not None:
                 stdout.close()
             if x:
@@ -1035,6 +1054,7 @@ def run(self, directory, with_output, run_args):
                 run_time, completed_fraction = last_run_info.split()
                 self._last_run_time = float(run_time)
                 self._last_run_completed_fraction = float(completed_fraction)
+        print("INFO _last_run_time = {} s".format(self._last_run_time))
 
         # Make sure that integration did not create NaN or very large values
         owners = [var.owner for var in self.arrays]
@@ -1362,6 +1382,14 @@ def network_run(self, net, duration, report=None, report_period=10*second,
         # We store this as an instance variable for later access by the
         # `code_object` method
         self.enable_profiling = profile
+
+        # To profile SpeedTests, we need to be able to set `profile` in
+        # `set_device`. Here we catch that case.
+        if 'profile' in self.build_options:
+            build_profile = self.build_options.pop('profile')
+            if build_profile:
+                self.enable_profiling = True
+
         all_objects = net.sorted_objects
         net._clocks = {obj.clock for obj in all_objects}
         t_end = net.t+duration
diff --git a/brian2/devices/cpp_standalone/templates/makefile b/brian2/devices/cpp_standalone/templates/makefile
index f3a9ca0f..671fd787 100644
--- a/brian2/devices/cpp_standalone/templates/makefile
+++ b/brian2/devices/cpp_standalone/templates/makefile
@@ -4,9 +4,8 @@ SRCS = {{source_files}}
 H_SRCS = {{header_files}}
 OBJS = ${SRCS:.cpp=.o}
 OBJS := ${OBJS:.c=.o}
-CC = @g++
 OPTIMISATIONS = {{ compiler_flags }}
-CFLAGS = -c -Wno-write-strings $(OPTIMISATIONS) -I. {{ openmp_pragma('compilation') }} {{ compiler_debug_flags }}
+CXXFLAGS = -c -Wno-write-strings $(OPTIMISATIONS) -I. {{ openmp_pragma('compilation') }} {{ compiler_debug_flags }}
 LFLAGS = {{ openmp_pragma('compilation') }} {{ linker_flags }} {{ linker_debug_flags }}
 DEPS = make.deps
 
@@ -15,17 +14,17 @@ all: $(PROGRAM)
 .PHONY: all clean
 
 $(PROGRAM): $(OBJS) $(DEPS) makefile
-	$(CC) $(OBJS) -o $(PROGRAM) $(LFLAGS)
+	$(CXX) $(OBJS) -o $(PROGRAM) $(LFLAGS)
 
 clean:
 	{{ rm_cmd }}
 
 make.deps: $(SRCS) $(H_SRCS)
-	$(CC) $(CFLAGS) -MM $(SRCS) > make.deps
+	$(CXX) $(CXXFLAGS) -MM $(SRCS) > make.deps
 	
 ifneq ($(wildcard $(DEPS)), )
 include $(DEPS)
 endif
 
 %.o : %.cpp makefile
-	$(CC) $(CFLAGS) $< -o $@
+	$(CXX) $(CXXFLAGS) $< -o $@
diff --git a/brian2/tests/features/base.py b/brian2/tests/features/base.py
index 03a81bfe..0a8a64c1 100644
--- a/brian2/tests/features/base.py
+++ b/brian2/tests/features/base.py
@@ -10,6 +10,7 @@
 import re
 
 from brian2.utils.stringtools import indent
+from brian2.core.base import BrianObjectException
 
 from collections import defaultdict
 
@@ -218,26 +219,47 @@ def after_run(self):
                             with_output=False)
     
     
-def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
-    tempfilename = tempfile.mktemp('exception')
+def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second,
+           profile_only_active=False, return_lrcf=False, prefs_dict={}):
+    exception_file = tempfile.NamedTemporaryFile(
+        prefix='brian_test_exception',
+        delete=False
+    )
+    exception_file.close()
+    tempfilename = exception_file.name
+    net_objects_file = tempfile.NamedTemporaryFile(
+        prefix='brian_test_network_objects', delete=False
+    )
+    net_objects_file.close()
+    tempfilename_net_obj = net_objects_file.name
     if n is None:
         init_args = ''
     else:
         init_args = str(n)
+    prefs_lines = []
+    for name, pref in prefs_dict.items():
+        prefs_lines.append(f"brian2.prefs['{name}'] = {pref}")
     code_string = '''
 __file__ = '{fname}'
 import brian2
-from {config_module} import {config_name}
+import {config_module}
 from {feature_module} import {feature_name}
-configuration = {config_name}()
+configuration = {config_module}.{config_name}()
 feature = {feature_name}({init_args})
 import warnings, traceback, pickle, sys, os, time
 warnings.simplefilter('ignore')
+{prefs_code}
 try:
     start_time = time.time()
     configuration.before_run()
     brian2.device._set_maximum_run_time({maximum_run_time})
     feature.run()
+    if {prof_active}:
+        code_objects = []
+        for obj in brian2.magic_network.objects:
+            if obj.active:
+                for codeobj in obj._code_objects:
+                    code_objects.append(codeobj.name)
     configuration.after_run()
     results = feature.results()
     run_time = time.time()-start_time
@@ -248,39 +270,84 @@ def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
             pass
     lrcf = configuration.get_last_run_completed_fraction()
     run_time = run_time/lrcf
-    prof_info = brian2.magic_network.profiling_info
     new_prof_info = []
-    for n, t in prof_info:
-        new_prof_info.append((n, t/lrcf))
+    try:
+        prof_info = brian2.magic_network.profiling_info
+        for n, t in prof_info:
+            new_prof_info.append((n, t/lrcf))
+    except ValueError:
+        pass
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((None, results, run_time, new_prof_info), f, -1)
+    pickle.dump((None, results, run_time, new_prof_info, lrcf), f, -1)
     f.close()
-except Exception, ex:
+    if {prof_active}:
+        f2 = open(r'{tempfname_net_obj}', 'wb')
+        pickle.dump(code_objects, f2, -1)
+        f2.close()
+except Exception as ex:
     #traceback.print_exc(file=sys.stdout)
     tb = traceback.format_exc()
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((tb, ex, 0.0, []), f, -1)
+    try:
+        pickle.dump((tb, ex, 0.0, [], 0.0), f, -1)
+    except pickle.PicklingError:
+        print(tb)
+        raise
     f.close()
+    if {prof_active}:
+        f2 = open(r'{tempfname_net_obj}', 'wb')
+        pickle.dump([], f2, -1)
+        f2.close()
     '''.format(config_module=configuration.__module__,
                config_name=configuration.__name__,
                feature_module=feature.__module__,
                feature_name=feature.__name__,
                tempfname=tempfilename,
+               tempfname_net_obj=tempfilename_net_obj,
                fname=__file__,
                init_args=init_args,
+               prefs_code="\n".join(prefs_lines),
                maximum_run_time=float(maximum_run_time),
+               prof_active=str(profile_only_active)
                )
     args = [sys.executable, '-c',
             code_string]
+    if hasattr(configuration, 'git_commit') and configuration.git_commit is not None:
+        # checkout the commit specified in the DynamicConfigCreator
+        configuration.git_checkout()
+        # checkout the original version of the module defining the feature
+        configuration.git_checkout_feature(feature.__module__)
+        configuration.git_checkout_feature(configuration.__module__)
     # Run the example in a new process and make sure that stdout gets
     # redirected into the capture plugin
     p = subprocess.Popen(args, stdout=subprocess.PIPE,
-                         stderr=subprocess.PIPE)
+                         stderr=subprocess.PIPE,
+                         encoding='UTF-8')
     stdout, stderr = p.communicate()
-    #sys.stdout.write(stdout)
-    #sys.stderr.write(stderr)
+    if p.returncode:
+        sys.stdout.write(stdout)
+        sys.stderr.write(stderr)
     with open(tempfilename, 'rb') as f:
-        tb, res, runtime, profiling_info = pickle.load(f)
+        tb, res, runtime, profiling_info, lrcf = pickle.load(f)
+    os.remove(tempfilename)
+    if isinstance(res, Exception):
+        tb = stdout + '\n' + stderr + '\n' + tb
+    else:
+        tb = stdout + '\n' + stderr
+    if profile_only_active:
+        with open(tempfilename_net_obj, 'rb') as f:
+            network_codeobjects = pickle.load(f)
+        os.remove(tempfilename_net_obj)
+        profiling_info = [(codeobj, time)
+                              for (codeobj, time) in profiling_info
+                              if codeobj in network_codeobjects]
+    if hasattr(configuration, 'git_commit') and configuration.git_commit is not None:
+        # reset the current changes before checking out original commit
+        configuration.git_reset()
+        # check out the original commit
+        configuration.git_checkout(reverse=True)
+    if return_lrcf:
+        return tb, res, runtime, profiling_info, lrcf
     return tb, res, runtime, profiling_info
     
 
@@ -325,7 +392,7 @@ def run_feature_tests(configurations=None, feature_tests=None,
             txt = 'OK'
             sym = '.'
             exc = None
-            tb, res, runtime, prof_info = results(configuration, ft, maximum_run_time=maximum_run_time)
+            tb, res, runtime, prof_info, lrcf = results(configuration, ft, maximum_run_time=maximum_run_time)
             if isinstance(res, Exception):
                 if isinstance(res, NotImplementedError):
                     sym = 'N'
@@ -469,7 +536,8 @@ def __str__(self):
 
 
 def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbose=True,
-                    n_slice=slice(None), maximum_run_time=1e7*brian2.second):
+                    n_slice=slice(None), maximum_run_time=1e7*brian2.second,
+                    profile_only_active=True, mark_not_completed=False):
     if configurations is None:
         # some configurations to attempt to import
         try:
@@ -487,15 +555,27 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
     full_results = {}
     tag_results = defaultdict(lambda:defaultdict(list))
     for ft in speed_tests:
+        traceback = {}
+        brian_stdouts = {}
+        result = {}
         if verbose:
             print(ft.fullname()+': ', end=' ')
+            sys.stdout.flush()
         for n in ft.n_range[n_slice]:
             if verbose:
                 print('n=%d [' % n, end=' ')
+                sys.stdout.flush()
             for configuration in configurations:
                 sym = '.'
+                brian_stdout = ''
                 for _ in range(1+int(run_twice)):
-                    tb, res, runtime, prof_info = results(configuration, ft, n, maximum_run_time=maximum_run_time)
+                    if mark_not_completed:
+                        tb, res, runtime, prof_info, lrcf = results(configuration, ft, n, maximum_run_time=maximum_run_time,
+                                                                    profile_only_active=profile_only_active,
+                                                                    return_lrcf=mark_not_completed)
+                    else:
+                        tb, res, runtime, prof_info = results(configuration, ft, n, maximum_run_time=maximum_run_time,
+                                                              profile_only_active=profile_only_active)
                 if isinstance(res, Exception):
                     if isinstance(res, NotImplementedError):
                         sym = 'N'
@@ -504,8 +584,28 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
                     if configuration is DefaultConfiguration:
                         raise res
                     runtime = numpy.NAN
+                    proj_dir = ''
+                    if configuration.name.startswith("CUDA"):
+                        proj_dir = 'cuda_standalone'
+                    elif configuration.name.startswith("CPP"):
+                        proj_dir = 'cpp_standalone'
+                    elif configuration.name.startswith("GeNN"):
+                        proj_dir = 'GeNNWorkspace'
+                    stdout_file = os.path.join(os.getcwd(), proj_dir, 'results/stdout.txt')
+                    if os.path.exists(stdout_file):
+                        with open(stdout_file, 'r') as sfile:
+                            brian_stdout = sfile.read()
+                    else:
+                        brian_stdout = 'no stdout file found, cwd = {}'.format(stdout_file)
                 sys.stdout.write(sym)
+                sys.stdout.flush()
                 full_results[configuration.name, ft.fullname(), n, 'All'] = runtime
+                if mark_not_completed:
+                    # save last run completed fraction
+                    full_results[configuration.name, ft.fullname(), n, 'lrcf'] = lrcf
+                traceback[configuration.name, ft.fullname(), n] = tb
+                brian_stdouts[configuration.name, ft.fullname(), n] = brian_stdout
+                result[configuration.name, n] = res
                 suffixtime = defaultdict(float)
                 overheadstime = float(runtime)
                 for codeobjname, proftime in prof_info:
@@ -520,18 +620,27 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
                 full_results[configuration.name, ft.fullname(), n, 'Overheads'] = overheadstime
             if verbose:
                 print(']', end=' ')
+                sys.stdout.flush()
         if verbose:
             print()
-        
-    return SpeedTestResults(full_results, configurations, speed_tests)
+            for n in ft.n_range[n_slice]:
+                for conf in configurations:
+                    if isinstance(result[conf.name, n], Exception):
+                        print("\nTRACEBACK {} N={}\n{}\n{}\n\n".format(conf.name, n,
+                                                                       brian_stdouts[conf.name, ft.fullname(), n],
+                                                                       traceback[conf.name, ft.fullname(), n]))
+
+    return SpeedTestResults(full_results, configurations, speed_tests, brian_stdouts, traceback)
 
 
 class SpeedTestResults(object):
-    def __init__(self, full_results, configurations, speed_tests):
+    def __init__(self, full_results, configurations, speed_tests, brian_stdouts, tracebacks):
         self.full_results = full_results
         self.configurations = configurations
         self.speed_tests = speed_tests
-        
+        self.brian_stdouts = brian_stdouts
+        self.tracebacks = tracebacks
+
     def get_ns(self, fullname):
         L = [(cn, fn, n, s)
              for cn, fn, n, s in self.full_results
@@ -546,7 +655,7 @@ def get_codeobjsuffixes(self, fullname):
         confignames, fullnames, n, codeobjsuffixes  = zip(*L)
         return set(codeobjsuffixes)
 
-    def plot_all_tests(self, relative=False, profiling_minimum=1.0):
+    def plot_all_tests(self, relative=False, profiling_minimum=1.0, print_relative=False):
         if relative and profiling_minimum<1:
             raise ValueError("Cannot use relative plots with profiling")
         import pylab
@@ -557,6 +666,8 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             codeobjsuffixes = self.get_codeobjsuffixes(fullname)
             codeobjsuffixes.remove('All')
             codeobjsuffixes.remove('Overheads')
+            if 'lrcf' in codeobjsuffixes:
+                codeobjsuffixes.remove('lrcf')
             codeobjsuffixes = ['All', 'Overheads']+sorted(codeobjsuffixes)
             if relative or profiling_minimum==1:
                 codeobjsuffixes = ['All']
@@ -566,31 +677,46 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             dashes = {}
             markerstyles = {}
             for isuffix, suffix in enumerate(codeobjsuffixes):
-                cols = itertools.cycle(pylab.rcParams['axes.color_cycle'])
-                for (iconfig, config), col in zip(enumerate(self.configurations), cols):
+                props = itertools.cycle(pylab.rcParams['axes.prop_cycle'])
+                for (iconfig, config), prop in zip(enumerate(self.configurations), props):
                     configname = config.name
                     runtimes = []
+                    not_finished = []
                     skip = True
                     for n in ns:
                         runtime = self.full_results.get((configname, fullname, n, 'All'), numpy.nan)
+                        if 'lrcf' in codeobjsuffixes:
+                            lrcf = self.full_results.get((configname, fullname, n, 'lrcf'), numpy.nan)
+                            not_finished.append(lrcf != 1.0)
+                        else:
+                            not_finished = [0]  # no plotting
                         thistime = self.full_results.get((configname, fullname, n, suffix), numpy.nan)
                         if float(thistime/runtime)>=profiling_minimum:
                             skip = False
                         runtimes.append(thistime)
+                        #overheadstime = self.full_results.get((configname, fullname, n, 'Overheads'), numpy.nan)
+                        #if (profiling_minimum<1 and  overheadstime == runtime:
+                        #    skip = True
                     if skip:
                         continue
                     runtimes = numpy.array(runtimes)
-                    if relative:
+                    if relative or print_relative:
                         if baseline is None:
                             baseline = runtimes
+                    if relative:
                         runtimes = baseline/runtimes
+                    if print_relative:
+                        rel = baseline/runtimes
+                        for ni, n in enumerate(ns):
+                            print("INFO relative performance for {ft} N={n} {conf}: {factor}".format(
+                                ft=fullname, n=n, conf=config.name, factor=rel[ni]))
                     if suffix=='All':
                         lw = 2
                         label = configname
                     else:
                         lw = 1
                         label = suffix
-                    plottable = sum(-numpy.isnan(runtimes[1:]+runtimes[:-1]))
+                    plottable = sum(~numpy.isnan(runtimes[1:]+runtimes[:-1]))
                     if plottable:
                         if label in havelabel:
                             label = None
@@ -612,8 +738,12 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                                         dash = dash+(4, 2)
                                 dashes[suffix] = dash
                                 markerstyles[suffix] = msty = next(markerstyles_cycle)
-                        line = pylab.plot(ns, runtimes, lw=lw, color=col, marker=msty,
+                        line = pylab.plot(ns, runtimes, lw=lw, color=prop['color'], marker=msty,
                                           mec='none', ms=8, label=label)[0]
+                        if suffix == 'All' and sum(not_finished) != 0:
+                            pylab.plot(ns[not_finished], runtimes[not_finished],
+                                       linestyle='None', marker=r'$\circlearrowleft$',
+                                       ms=15, color=prop['color'], label='linear runtime interpolation')
                         if dash is not None:
                             line.set_dashes(dash)
             pylab.title(fullname)
@@ -623,6 +753,7 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                 pylab.gca().set_xscale('log')
             if st.time_axis_log:
                 pylab.gca().set_yscale('log')
+            pylab.grid(True, which='both')
 
 # Code below auto generates restructured text tables, copied from:
 # http://stackoverflow.com/questions/11347505/what-are-some-approaches-to-outputting-a-python-data-structure-to-restructuredte
diff --git a/brian2/tests/features/speed.py b/brian2/tests/features/speed.py
index 317bbf6d..0b6ba2f6 100644
--- a/brian2/tests/features/speed.py
+++ b/brian2/tests/features/speed.py
@@ -23,7 +23,7 @@ class LinearNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Linear 1D"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000, 1000000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 261015625]  #fail: 262031250
     n_label = 'Num neurons'
 
     # configuration options
@@ -42,7 +42,7 @@ class HHNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Hodgkin-Huxley"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 102750000]  #fail: 103125000
     n_label = 'Num neurons'
 
     # configuration options
@@ -86,7 +86,7 @@ class CUBAFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "CUBA fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 3546875]  #fail: 3562500
     n_label = 'Num neurons'
 
     # configuration options
@@ -132,7 +132,7 @@ class COBAHHFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "COBAHH fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]
+    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 3781250]  #fail: 3812500
     n_label = 'Num neurons'
 
     # configuration options
@@ -255,7 +255,7 @@ def run(self):
 class SynapsesOnly(object):
     category = "Synapses only"
     tags = ["Synapses"]
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000]
     n_label = 'Num neurons'
     duration = 1 * second
     # memory usage will be approximately p**2*rate*dt*N**2*bytes_per_synapse/1024**3 GB
@@ -282,7 +282,7 @@ class VerySparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Very sparse, medium rate (10s duration)"
     rate = 10 * Hz
     p = 0.02
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 3875000]  #fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -290,21 +290,21 @@ class SparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, medium rate (1s duration)"
     rate = 10 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 1234375]  #fail: 1242187  # weave max CPU time should be about 5m
 
 
 class DenseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Dense, medium rate (1s duration)"
     rate = 10 * Hz
     p = 1.0
-    n_range = [10, 100, 1000, 10000, 40000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 546875]  #fail: 554687  # weave max CPU time should be about 4m
 
 
 class SparseLowRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, low rate (10s duration)"
     rate = 1 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 3875000]  #fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -312,7 +312,7 @@ class SparseHighRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, high rate (1s duration)"
     rate = 100 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 387500]  #fail: 393750  # weave max CPU time should be about 5m
 
 
 if __name__ == '__main__':
diff --git a/brian2/tests/test_preferences.py b/brian2/tests/test_preferences.py
index 1c2941f9..9ba7e72f 100644
--- a/brian2/tests/test_preferences.py
+++ b/brian2/tests/test_preferences.py
@@ -169,6 +169,9 @@ def test_brianglobalpreferences():
     # check that load_preferences works, but nothing about its values
     gp = BrianGlobalPreferences()
     gp.load_preferences()
+    # Check that resetting to default preferences works
+    gp = BrianGlobalPreferences()
+    gp.reset_to_defaults()
 
 
 @pytest.mark.codegen_independent
