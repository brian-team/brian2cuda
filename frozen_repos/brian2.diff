diff --git a/brian2/devices/cpp_standalone/device.py b/brian2/devices/cpp_standalone/device.py
index 54aeccf5..42da591c 100644
--- a/brian2/devices/cpp_standalone/device.py
+++ b/brian2/devices/cpp_standalone/device.py
@@ -1055,6 +1055,7 @@ def run(self, directory, with_output, run_args):
                 run_time, completed_fraction = last_run_info.split()
                 self._last_run_time = float(run_time)
                 self._last_run_completed_fraction = float(completed_fraction)
+        print("INFO _last_run_time = {} s".format(self._last_run_time))
 
         # Make sure that integration did not create NaN or very large values
         owners = [var.owner for var in self.arrays]
diff --git a/brian2/tests/features/base.py b/brian2/tests/features/base.py
index 03a81bfe..83402d97 100644
--- a/brian2/tests/features/base.py
+++ b/brian2/tests/features/base.py
@@ -10,6 +10,7 @@
 import re
 
 from brian2.utils.stringtools import indent
+from brian2.core.base import BrianObjectException
 
 from collections import defaultdict
 
@@ -218,8 +219,10 @@ def after_run(self):
                             with_output=False)
     
     
-def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
-    tempfilename = tempfile.mktemp('exception')
+def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second,
+           profile_only_active=False, return_lrcf=False):
+    tempfilename = 'my_file_1'#tempfile.mktemp('exception')
+    tempfilename_net_obj = 'my_file_2'#tempfile.mktemp('network_objects')
     if n is None:
         init_args = ''
     else:
@@ -227,9 +230,9 @@ def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
     code_string = '''
 __file__ = '{fname}'
 import brian2
-from {config_module} import {config_name}
+import {config_module}
 from {feature_module} import {feature_name}
-configuration = {config_name}()
+configuration = {config_module}.{config_name}()
 feature = {feature_name}({init_args})
 import warnings, traceback, pickle, sys, os, time
 warnings.simplefilter('ignore')
@@ -238,6 +241,12 @@ def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
     configuration.before_run()
     brian2.device._set_maximum_run_time({maximum_run_time})
     feature.run()
+    if {prof_active}:
+        code_objects = []
+        for obj in brian2.magic_network.objects:
+            if obj.active:
+                for codeobj in obj._code_objects:
+                    code_objects.append(codeobj.name)
     configuration.after_run()
     results = feature.results()
     run_time = time.time()-start_time
@@ -248,39 +257,80 @@ def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
             pass
     lrcf = configuration.get_last_run_completed_fraction()
     run_time = run_time/lrcf
-    prof_info = brian2.magic_network.profiling_info
     new_prof_info = []
-    for n, t in prof_info:
-        new_prof_info.append((n, t/lrcf))
+    try:
+        prof_info = brian2.magic_network.profiling_info
+        for n, t in prof_info:
+            new_prof_info.append((n, t/lrcf))
+    except ValueError:
+        pass
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((None, results, run_time, new_prof_info), f, -1)
+    pickle.dump((None, results, run_time, new_prof_info, lrcf), f, -1)
     f.close()
+    if {prof_active}:
+        f2 = open(r'{tempfname_net_obj}', 'wb')
+        pickle.dump(code_objects, f2, -1)
+        f2.close()
 except Exception, ex:
     #traceback.print_exc(file=sys.stdout)
     tb = traceback.format_exc()
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((tb, ex, 0.0, []), f, -1)
+    try:
+        pickle.dump((tb, ex, 0.0, [], 0.0), f, -1)
+    except pickle.PicklingError:
+        print tb
+        raise
     f.close()
+    if {prof_active}:
+        f2 = open(r'{tempfname_net_obj}', 'wb')
+        pickle.dump([], f2, -1)
+        f2.close()
     '''.format(config_module=configuration.__module__,
                config_name=configuration.__name__,
                feature_module=feature.__module__,
                feature_name=feature.__name__,
                tempfname=tempfilename,
+               tempfname_net_obj=tempfilename_net_obj,
                fname=__file__,
                init_args=init_args,
                maximum_run_time=float(maximum_run_time),
+               prof_active=str(profile_only_active)
                )
     args = [sys.executable, '-c',
             code_string]
+    if hasattr(configuration, 'git_commit') and configuration.git_commit is not None:
+        # checkout the commit specified in the DynamicConfigCreator
+        configuration.git_checkout()
+        # checkout the original version of the module defining the feature
+        configuration.git_checkout_feature(feature.__module__)
+        configuration.git_checkout_feature(configuration.__module__)
     # Run the example in a new process and make sure that stdout gets
     # redirected into the capture plugin
     p = subprocess.Popen(args, stdout=subprocess.PIPE,
                          stderr=subprocess.PIPE)
     stdout, stderr = p.communicate()
-    #sys.stdout.write(stdout)
-    #sys.stderr.write(stderr)
+    if p.returncode:
+        sys.stdout.write(stdout)
+        sys.stderr.write(stderr)
     with open(tempfilename, 'rb') as f:
-        tb, res, runtime, profiling_info = pickle.load(f)
+        tb, res, runtime, profiling_info, lrcf = pickle.load(f)
+    if isinstance(res, Exception):
+        tb = stdout + '\n' + stderr + '\n' + tb
+    else:
+        tb = stdout + '\n' + stderr
+    if profile_only_active:
+        with open(tempfilename_net_obj, 'rb') as f:
+            network_codeobjects = pickle.load(f)
+        profiling_info = [(codeobj, time)
+                              for (codeobj, time) in profiling_info
+                              if codeobj in network_codeobjects]
+    if hasattr(configuration, 'git_commit') and configuration.git_commit is not None:
+        # reset the current changes before checking out original commit
+        configuration.git_reset()
+        # check out the original commit
+        configuration.git_checkout(reverse=True)
+    if return_lrcf:
+        return tb, res, runtime, profiling_info, lrcf
     return tb, res, runtime, profiling_info
     
 
@@ -325,7 +375,7 @@ def run_feature_tests(configurations=None, feature_tests=None,
             txt = 'OK'
             sym = '.'
             exc = None
-            tb, res, runtime, prof_info = results(configuration, ft, maximum_run_time=maximum_run_time)
+            tb, res, runtime, prof_info, lrcf = results(configuration, ft, maximum_run_time=maximum_run_time)
             if isinstance(res, Exception):
                 if isinstance(res, NotImplementedError):
                     sym = 'N'
@@ -469,7 +519,8 @@ def __str__(self):
 
 
 def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbose=True,
-                    n_slice=slice(None), maximum_run_time=1e7*brian2.second):
+                    n_slice=slice(None), maximum_run_time=1e7*brian2.second,
+                    profile_only_active=True, mark_not_completed=False):
     if configurations is None:
         # some configurations to attempt to import
         try:
@@ -487,15 +538,27 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
     full_results = {}
     tag_results = defaultdict(lambda:defaultdict(list))
     for ft in speed_tests:
+        traceback = {}
+        brian_stdouts = {}
+        result = {}
         if verbose:
             print(ft.fullname()+': ', end=' ')
+            sys.stdout.flush()
         for n in ft.n_range[n_slice]:
             if verbose:
                 print('n=%d [' % n, end=' ')
+                sys.stdout.flush()
             for configuration in configurations:
                 sym = '.'
+                brian_stdout = ''
                 for _ in range(1+int(run_twice)):
-                    tb, res, runtime, prof_info = results(configuration, ft, n, maximum_run_time=maximum_run_time)
+                    if mark_not_completed:
+                        tb, res, runtime, prof_info, lrcf = results(configuration, ft, n, maximum_run_time=maximum_run_time,
+                                                                    profile_only_active=profile_only_active,
+                                                                    return_lrcf=mark_not_completed)
+                    else:
+                        tb, res, runtime, prof_info = results(configuration, ft, n, maximum_run_time=maximum_run_time,
+                                                              profile_only_active=profile_only_active)
                 if isinstance(res, Exception):
                     if isinstance(res, NotImplementedError):
                         sym = 'N'
@@ -504,8 +567,28 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
                     if configuration is DefaultConfiguration:
                         raise res
                     runtime = numpy.NAN
+                    proj_dir = ''
+                    if configuration.name.startswith("CUDA"):
+                        proj_dir = 'cuda_standalone'
+                    elif configuration.name.startswith("CPP"):
+                        proj_dir = 'cpp_standalone'
+                    elif configuration.name.startswith("GeNN"):
+                        proj_dir = 'GeNNWorkspace'
+                    stdout_file = os.path.join(os.getcwd(), proj_dir, 'results/stdout.txt')
+                    if os.path.exists(stdout_file):
+                        with open(stdout_file, 'r') as sfile:
+                            brian_stdout = sfile.read()
+                    else:
+                        brian_stdout = 'no stdout file found, cwd = {}'.format(stdout_file)
                 sys.stdout.write(sym)
+                sys.stdout.flush()
                 full_results[configuration.name, ft.fullname(), n, 'All'] = runtime
+                if mark_not_completed:
+                    # save last run completed fraction
+                    full_results[configuration.name, ft.fullname(), n, 'lrcf'] = lrcf
+                traceback[configuration.name, ft.fullname(), n] = tb
+                brian_stdouts[configuration.name, ft.fullname(), n] = brian_stdout
+                result[configuration.name, n] = res
                 suffixtime = defaultdict(float)
                 overheadstime = float(runtime)
                 for codeobjname, proftime in prof_info:
@@ -520,18 +603,27 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
                 full_results[configuration.name, ft.fullname(), n, 'Overheads'] = overheadstime
             if verbose:
                 print(']', end=' ')
+                sys.stdout.flush()
         if verbose:
             print()
-        
-    return SpeedTestResults(full_results, configurations, speed_tests)
+            for n in ft.n_range[n_slice]:
+                for conf in configurations:
+                    if isinstance(result[conf.name, n], Exception):
+                        print("\nTRACEBACK {} N={}\n{}\n{}\n\n".format(conf.name, n,
+                                                                       brian_stdouts[conf.name, ft.fullname(), n],
+                                                                       traceback[conf.name, ft.fullname(), n]))
+
+    return SpeedTestResults(full_results, configurations, speed_tests, brian_stdouts, traceback)
 
 
 class SpeedTestResults(object):
-    def __init__(self, full_results, configurations, speed_tests):
+    def __init__(self, full_results, configurations, speed_tests, brian_stdouts, tracebacks):
         self.full_results = full_results
         self.configurations = configurations
         self.speed_tests = speed_tests
-        
+        self.brian_stdouts = brian_stdouts
+        self.tracebacks = tracebacks
+
     def get_ns(self, fullname):
         L = [(cn, fn, n, s)
              for cn, fn, n, s in self.full_results
@@ -546,7 +638,7 @@ def get_codeobjsuffixes(self, fullname):
         confignames, fullnames, n, codeobjsuffixes  = zip(*L)
         return set(codeobjsuffixes)
 
-    def plot_all_tests(self, relative=False, profiling_minimum=1.0):
+    def plot_all_tests(self, relative=False, profiling_minimum=1.0, print_relative=False):
         if relative and profiling_minimum<1:
             raise ValueError("Cannot use relative plots with profiling")
         import pylab
@@ -557,6 +649,8 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             codeobjsuffixes = self.get_codeobjsuffixes(fullname)
             codeobjsuffixes.remove('All')
             codeobjsuffixes.remove('Overheads')
+            if 'lrcf' in codeobjsuffixes:
+                codeobjsuffixes.remove('lrcf')
             codeobjsuffixes = ['All', 'Overheads']+sorted(codeobjsuffixes)
             if relative or profiling_minimum==1:
                 codeobjsuffixes = ['All']
@@ -566,31 +660,46 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             dashes = {}
             markerstyles = {}
             for isuffix, suffix in enumerate(codeobjsuffixes):
-                cols = itertools.cycle(pylab.rcParams['axes.color_cycle'])
-                for (iconfig, config), col in zip(enumerate(self.configurations), cols):
+                props = itertools.cycle(pylab.rcParams['axes.prop_cycle'])
+                for (iconfig, config), prop in zip(enumerate(self.configurations), props):
                     configname = config.name
                     runtimes = []
+                    not_finished = []
                     skip = True
                     for n in ns:
                         runtime = self.full_results.get((configname, fullname, n, 'All'), numpy.nan)
+                        if 'lrcf' in codeobjsuffixes:
+                            lrcf = self.full_results.get((configname, fullname, n, 'lrcf'), numpy.nan)
+                            not_finished.append(lrcf != 1.0)
+                        else:
+                            not_finished = [0]  # no plotting
                         thistime = self.full_results.get((configname, fullname, n, suffix), numpy.nan)
                         if float(thistime/runtime)>=profiling_minimum:
                             skip = False
                         runtimes.append(thistime)
+                        #overheadstime = self.full_results.get((configname, fullname, n, 'Overheads'), numpy.nan)
+                        #if (profiling_minimum<1 and  overheadstime == runtime:
+                        #    skip = True
                     if skip:
                         continue
                     runtimes = numpy.array(runtimes)
-                    if relative:
+                    if relative or print_relative:
                         if baseline is None:
                             baseline = runtimes
+                    if relative:
                         runtimes = baseline/runtimes
+                    if print_relative:
+                        rel = baseline/runtimes
+                        for ni, n in enumerate(ns):
+                            print("INFO relative performance for {ft} N={n} {conf}: {factor}".format(
+                                ft=fullname, n=n, conf=config.name, factor=rel[ni]))
                     if suffix=='All':
                         lw = 2
                         label = configname
                     else:
                         lw = 1
                         label = suffix
-                    plottable = sum(-numpy.isnan(runtimes[1:]+runtimes[:-1]))
+                    plottable = sum(~numpy.isnan(runtimes[1:]+runtimes[:-1]))
                     if plottable:
                         if label in havelabel:
                             label = None
@@ -612,8 +721,12 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                                         dash = dash+(4, 2)
                                 dashes[suffix] = dash
                                 markerstyles[suffix] = msty = next(markerstyles_cycle)
-                        line = pylab.plot(ns, runtimes, lw=lw, color=col, marker=msty,
+                        line = pylab.plot(ns, runtimes, lw=lw, color=prop['color'], marker=msty,
                                           mec='none', ms=8, label=label)[0]
+                        if suffix == 'All' and sum(not_finished) != 0:
+                            pylab.plot(ns[not_finished], runtimes[not_finished],
+                                       linestyle='None', marker=r'$\circlearrowleft$',
+                                       ms=15, color=prop['color'], label='linear runtime interpolation')
                         if dash is not None:
                             line.set_dashes(dash)
             pylab.title(fullname)
@@ -623,6 +736,7 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                 pylab.gca().set_xscale('log')
             if st.time_axis_log:
                 pylab.gca().set_yscale('log')
+            pylab.grid(True, which='both')
 
 # Code below auto generates restructured text tables, copied from:
 # http://stackoverflow.com/questions/11347505/what-are-some-approaches-to-outputting-a-python-data-structure-to-restructuredte
diff --git a/brian2/tests/features/speed.py b/brian2/tests/features/speed.py
index eedb27a6..466d173b 100644
--- a/brian2/tests/features/speed.py
+++ b/brian2/tests/features/speed.py
@@ -23,7 +23,7 @@ class LinearNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Linear 1D"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000, 1000000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 261015625]  #fail: 262031250
     n_label = 'Num neurons'
 
     # configuration options
@@ -42,7 +42,7 @@ class HHNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Hodgkin-Huxley"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 102750000]  #fail: 103125000
     n_label = 'Num neurons'
 
     # configuration options
@@ -86,7 +86,7 @@ class CUBAFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "CUBA fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 3546875]  #fail: 3562500
     n_label = 'Num neurons'
 
     # configuration options
@@ -132,7 +132,7 @@ class COBAHHFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "COBAHH fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]
+    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 3781250]  #fail: 3812500
     n_label = 'Num neurons'
 
     # configuration options
@@ -255,7 +255,7 @@ def run(self):
 class SynapsesOnly(object):
     category = "Synapses only"
     tags = ["Synapses"]
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000]
     n_label = 'Num neurons'
     duration = 1 * second
     # memory usage will be approximately p**2*rate*dt*N**2*bytes_per_synapse/1024**3 GB
@@ -282,7 +282,7 @@ class VerySparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Very sparse, medium rate (10s duration)"
     rate = 10 * Hz
     p = 0.02
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 3875000]  #fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -290,21 +290,21 @@ class SparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, medium rate (1s duration)"
     rate = 10 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 1234375]  #fail: 1242187  # weave max CPU time should be about 5m
 
 
 class DenseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Dense, medium rate (1s duration)"
     rate = 10 * Hz
     p = 1.0
-    n_range = [10, 100, 1000, 10000, 40000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 546875]  #fail: 554687  # weave max CPU time should be about 4m
 
 
 class SparseLowRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, low rate (10s duration)"
     rate = 1 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 3875000]  #fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -312,7 +312,7 @@ class SparseHighRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, high rate (1s duration)"
     rate = 100 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 387500]  #fail: 393750  # weave max CPU time should be about 5m
 
 
 if __name__ == '__main__':
