diff --git a/brian2/devices/cpp_standalone/device.py b/brian2/devices/cpp_standalone/device.py
index 7cb100b9..10f878c6 100644
--- a/brian2/devices/cpp_standalone/device.py
+++ b/brian2/devices/cpp_standalone/device.py
@@ -1341,6 +1341,7 @@ def run(
                 run_time, completed_fraction = last_run_info.split()
                 self._last_run_time = float(run_time)
                 self._last_run_completed_fraction = float(completed_fraction)
+        print(f"INFO _last_run_time = {self._last_run_time} s")
 
         # Make sure that integration did not create NaN or very large values
         owners = [var.owner for var in self.arrays]
diff --git a/brian2/tests/features/__init__.py b/brian2/tests/features/__init__.py
index 9ad11c0b..7442aaf7 100644
--- a/brian2/tests/features/__init__.py
+++ b/brian2/tests/features/__init__.py
@@ -5,6 +5,7 @@
     "Configuration",
     "run_feature_tests",
 ]
+
 # isort: skip_file # We need to do the base import first to prevent a circular import later
 from .base import *
 from . import input, monitors, neurongroup, speed, synapses
diff --git a/brian2/tests/features/base.py b/brian2/tests/features/base.py
index 1f19da86..ee692cde 100644
--- a/brian2/tests/features/base.py
+++ b/brian2/tests/features/base.py
@@ -11,6 +11,7 @@
 import numpy
 
 import brian2
+from brian2.core.base import BrianObjectException
 from brian2.utils.stringtools import indent
 
 __all__ = [
@@ -232,8 +233,16 @@ def after_run(self):
         )
 
 
-def results(configuration, feature, n=None, maximum_run_time=1e7 * brian2.second):
-    tempfilename = tempfile.mktemp("exception")
+def results(
+    configuration,
+    feature,
+    n=None,
+    maximum_run_time=1e7 * brian2.second,
+    profile_only_active=False,
+    return_lrcf=False,
+):
+    tempfilename = "my_file_1"  # tempfile.mktemp('exception')
+    tempfilename_net_obj = "my_file_2"  # tempfile.mktemp('network_objects')
     if n is None:
         init_args = ""
     else:
@@ -241,9 +250,9 @@ def results(configuration, feature, n=None, maximum_run_time=1e7 * brian2.second
     code_string = """
 __file__ = '{fname}'
 import brian2
-from {config_module} import {config_name}
+import {config_module}
 from {feature_module} import {feature_name}
-configuration = {config_name}()
+configuration = {config_module}.{config_name}()
 feature = {feature_name}({init_args})
 import warnings, traceback, pickle, sys, os, time
 warnings.simplefilter('ignore')
@@ -252,6 +261,12 @@ def results(configuration, feature, n=None, maximum_run_time=1e7 * brian2.second
     configuration.before_run()
     brian2.device._set_maximum_run_time({maximum_run_time})
     feature.run()
+    if {prof_active}:
+        code_objects = []
+        for obj in brian2.magic_network.objects:
+            if obj.active:
+                for codeobj in obj._code_objects:
+                    code_objects.append(codeobj.name)
     configuration.after_run()
     results = feature.results()
     run_time = time.time()-start_time
@@ -262,38 +277,81 @@ def results(configuration, feature, n=None, maximum_run_time=1e7 * brian2.second
             pass
     lrcf = configuration.get_last_run_completed_fraction()
     run_time = run_time/lrcf
-    prof_info = brian2.magic_network.profiling_info
     new_prof_info = []
-    for n, t in prof_info:
-        new_prof_info.append((n, t/lrcf))
+    try:
+        prof_info = brian2.magic_network.profiling_info
+        for n, t in prof_info:
+            new_prof_info.append((n, t/lrcf))
+    except ValueError:
+        pass
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((None, results, run_time, new_prof_info), f, -1)
+    pickle.dump((None, results, run_time, new_prof_info, lrcf), f, -1)
     f.close()
+    if {prof_active}:
+        f2 = open(r'{tempfname_net_obj}', 'wb')
+        pickle.dump(code_objects, f2, -1)
+        f2.close()
 except Exception, ex:
     #traceback.print_exc(file=sys.stdout)
     tb = traceback.format_exc()
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((tb, ex, 0.0, []), f, -1)
+    try:
+        pickle.dump((tb, ex, 0.0, [], 0.0), f, -1)
+    except pickle.PicklingError:
+        print tb
+        raise
     f.close()
+    if {prof_active}:
+        f2 = open(r'{tempfname_net_obj}', 'wb')
+        pickle.dump([], f2, -1)
+        f2.close()
     """.format(
         config_module=configuration.__module__,
         config_name=configuration.__name__,
         feature_module=feature.__module__,
         feature_name=feature.__name__,
         tempfname=tempfilename,
+        tempfname_net_obj=tempfilename_net_obj,
         fname=__file__,
         init_args=init_args,
         maximum_run_time=float(maximum_run_time),
+        prof_active=str(profile_only_active),
     )
     args = [sys.executable, "-c", code_string]
+    if hasattr(configuration, "git_commit") and configuration.git_commit is not None:
+        # checkout the commit specified in the DynamicConfigCreator
+        configuration.git_checkout()
+        # checkout the original version of the module defining the feature
+        configuration.git_checkout_feature(feature.__module__)
+        configuration.git_checkout_feature(configuration.__module__)
     # Run the example in a new process and make sure that stdout gets
     # redirected into the capture plugin
     p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
     stdout, stderr = p.communicate()
-    # sys.stdout.write(stdout)
-    # sys.stderr.write(stderr)
+    if p.returncode:
+        sys.stdout.write(stdout)
+        sys.stderr.write(stderr)
     with open(tempfilename, "rb") as f:
-        tb, res, runtime, profiling_info = pickle.load(f)
+        tb, res, runtime, profiling_info, lrcf = pickle.load(f)
+    if isinstance(res, Exception):
+        tb = stdout + "\n" + stderr + "\n" + tb
+    else:
+        tb = stdout + "\n" + stderr
+    if profile_only_active:
+        with open(tempfilename_net_obj, "rb") as f:
+            network_codeobjects = pickle.load(f)
+        profiling_info = [
+            (codeobj, time)
+            for (codeobj, time) in profiling_info
+            if codeobj in network_codeobjects
+        ]
+    if hasattr(configuration, "git_commit") and configuration.git_commit is not None:
+        # reset the current changes before checking out original commit
+        configuration.git_reset()
+        # check out the original commit
+        configuration.git_checkout(reverse=True)
+    if return_lrcf:
+        return tb, res, runtime, profiling_info, lrcf
     return tb, res, runtime, profiling_info
 
 
@@ -344,7 +402,7 @@ def run_feature_tests(
             txt = "OK"
             sym = "."
             exc = None
-            tb, res, runtime, prof_info = results(
+            tb, res, runtime, prof_info, lrcf = results(
                 configuration, ft, maximum_run_time=maximum_run_time
             )
             if isinstance(res, Exception):
@@ -511,6 +569,8 @@ def run_speed_tests(
     verbose=True,
     n_slice=slice(None),
     maximum_run_time=1e7 * brian2.second,
+    profile_only_active=True,
+    mark_not_completed=False,
 ):
     if configurations is None:
         # some configurations to attempt to import
@@ -529,17 +589,37 @@ def run_speed_tests(
     full_results = {}
     tag_results = defaultdict(lambda: defaultdict(list))
     for ft in speed_tests:
+        traceback = {}
+        brian_stdouts = {}
+        result = {}
         if verbose:
             print(f"{ft.fullname()}: ", end=" ")
+            sys.stdout.flush()
         for n in ft.n_range[n_slice]:
             if verbose:
                 print(f"n={int(n)} [", end=" ")
+                sys.stdout.flush()
             for configuration in configurations:
                 sym = "."
+                brian_stdout = ""
                 for _ in range(1 + int(run_twice)):
-                    tb, res, runtime, prof_info = results(
-                        configuration, ft, n, maximum_run_time=maximum_run_time
-                    )
+                    if mark_not_completed:
+                        tb, res, runtime, prof_info, lrcf = results(
+                            configuration,
+                            ft,
+                            n,
+                            maximum_run_time=maximum_run_time,
+                            profile_only_active=profile_only_active,
+                            return_lrcf=mark_not_completed,
+                        )
+                    else:
+                        tb, res, runtime, prof_info = results(
+                            configuration,
+                            ft,
+                            n,
+                            maximum_run_time=maximum_run_time,
+                            profile_only_active=profile_only_active,
+                        )
                 if isinstance(res, Exception):
                     if isinstance(res, NotImplementedError):
                         sym = "N"
@@ -548,8 +628,32 @@ def run_speed_tests(
                     if configuration is DefaultConfiguration:
                         raise res
                     runtime = numpy.NAN
+                    proj_dir = ""
+                    if configuration.name.startswith("CUDA"):
+                        proj_dir = "cuda_standalone"
+                    elif configuration.name.startswith("CPP"):
+                        proj_dir = "cpp_standalone"
+                    elif configuration.name.startswith("GeNN"):
+                        proj_dir = "GeNNWorkspace"
+                    stdout_file = os.path.join(
+                        os.getcwd(), proj_dir, "results/stdout.txt"
+                    )
+                    if os.path.exists(stdout_file):
+                        with open(stdout_file) as sfile:
+                            brian_stdout = sfile.read()
+                    else:
+                        brian_stdout = "no stdout file found, cwd = {}".format(
+                            stdout_file
+                        )
                 sys.stdout.write(sym)
+                sys.stdout.flush()
                 full_results[configuration.name, ft.fullname(), n, "All"] = runtime
+                if mark_not_completed:
+                    # save last run completed fraction
+                    full_results[configuration.name, ft.fullname(), n, "lrcf"] = lrcf
+                traceback[configuration.name, ft.fullname(), n] = tb
+                brian_stdouts[configuration.name, ft.fullname(), n] = brian_stdout
+                result[configuration.name, n] = res
                 suffixtime = defaultdict(float)
                 overheadstime = float(runtime)
                 for codeobjname, proftime in prof_info:
@@ -568,17 +672,35 @@ def run_speed_tests(
                 )
             if verbose:
                 print("]", end=" ")
+                sys.stdout.flush()
         if verbose:
             print()
+            for n in ft.n_range[n_slice]:
+                for conf in configurations:
+                    if isinstance(result[conf.name, n], Exception):
+                        print(
+                            "\nTRACEBACK {} N={}\n{}\n{}\n\n".format(
+                                conf.name,
+                                n,
+                                brian_stdouts[conf.name, ft.fullname(), n],
+                                traceback[conf.name, ft.fullname(), n],
+                            )
+                        )
 
-    return SpeedTestResults(full_results, configurations, speed_tests)
+    return SpeedTestResults(
+        full_results, configurations, speed_tests, brian_stdouts, traceback
+    )
 
 
 class SpeedTestResults:
-    def __init__(self, full_results, configurations, speed_tests):
+    def __init__(
+        self, full_results, configurations, speed_tests, brian_stdouts, tracebacks
+    ):
         self.full_results = full_results
         self.configurations = configurations
         self.speed_tests = speed_tests
+        self.brian_stdouts = brian_stdouts
+        self.tracebacks = tracebacks
 
     def get_ns(self, fullname):
         L = [(cn, fn, n, s) for cn, fn, n, s in self.full_results if fn == fullname]
@@ -590,7 +712,9 @@ def get_codeobjsuffixes(self, fullname):
         confignames, fullnames, n, codeobjsuffixes = zip(*L)
         return set(codeobjsuffixes)
 
-    def plot_all_tests(self, relative=False, profiling_minimum=1.0):
+    def plot_all_tests(
+        self, relative=False, profiling_minimum=1.0, print_relative=False
+    ):
         if relative and profiling_minimum < 1:
             raise ValueError("Cannot use relative plots with profiling")
         import pylab
@@ -602,6 +726,8 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             codeobjsuffixes = self.get_codeobjsuffixes(fullname)
             codeobjsuffixes.remove("All")
             codeobjsuffixes.remove("Overheads")
+            if "lrcf" in codeobjsuffixes:
+                codeobjsuffixes.remove("lrcf")
             codeobjsuffixes = ["All", "Overheads"] + sorted(codeobjsuffixes)
             if relative or profiling_minimum == 1:
                 codeobjsuffixes = ["All"]
@@ -613,35 +739,58 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             dashes = {}
             markerstyles = {}
             for isuffix, suffix in enumerate(codeobjsuffixes):
-                cols = itertools.cycle(pylab.rcParams["axes.color_cycle"])
-                for (iconfig, config), col in zip(enumerate(self.configurations), cols):
+                props = itertools.cycle(pylab.rcParams["axes.prop_cycle"])
+                for (iconfig, config), prop in zip(
+                    enumerate(self.configurations), props
+                ):
                     configname = config.name
                     runtimes = []
+                    not_finished = []
                     skip = True
                     for n in ns:
                         runtime = self.full_results.get(
                             (configname, fullname, n, "All"), numpy.nan
                         )
+                        if "lrcf" in codeobjsuffixes:
+                            lrcf = self.full_results.get(
+                                (configname, fullname, n, "lrcf"), numpy.nan
+                            )
+                            not_finished.append(lrcf != 1.0)
+                        else:
+                            not_finished = [0]  # no plotting
                         thistime = self.full_results.get(
                             (configname, fullname, n, suffix), numpy.nan
                         )
                         if float(thistime / runtime) >= profiling_minimum:
                             skip = False
                         runtimes.append(thistime)
+                        # overheadstime = self.full_results.get((configname, fullname, n, 'Overheads'), numpy.nan)
+                        # if (profiling_minimum<1 and  overheadstime == runtime:
+                        #    skip = True
                     if skip:
                         continue
                     runtimes = numpy.array(runtimes)
-                    if relative:
+                    if relative or print_relative:
                         if baseline is None:
                             baseline = runtimes
+                    if relative:
                         runtimes = baseline / runtimes
+                    if print_relative:
+                        rel = baseline / runtimes
+                        for ni, n in enumerate(ns):
+                            print(
+                                "INFO relative performance for {ft} N={n} {conf}:"
+                                " {factor}".format(
+                                    ft=fullname, n=n, conf=config.name, factor=rel[ni]
+                                )
+                            )
                     if suffix == "All":
                         lw = 2
                         label = configname
                     else:
                         lw = 1
                         label = suffix
-                    plottable = sum(-numpy.isnan(runtimes[1:] + runtimes[:-1]))
+                    plottable = sum(~numpy.isnan(runtimes[1:] + runtimes[:-1]))
                     if plottable:
                         if label in havelabel:
                             label = None
@@ -667,12 +816,22 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                             ns,
                             runtimes,
                             lw=lw,
-                            color=col,
+                            color=prop["color"],
                             marker=msty,
                             mec="none",
                             ms=8,
                             label=label,
                         )[0]
+                        if suffix == "All" and sum(not_finished) != 0:
+                            pylab.plot(
+                                ns[not_finished],
+                                runtimes[not_finished],
+                                linestyle="None",
+                                marker=r"$\circlearrowleft$",
+                                ms=15,
+                                color=prop["color"],
+                                label="linear runtime interpolation",
+                            )
                         if dash is not None:
                             line.set_dashes(dash)
             pylab.title(fullname)
@@ -682,6 +841,7 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                 pylab.gca().set_xscale("log")
             if st.time_axis_log:
                 pylab.gca().set_yscale("log")
+            pylab.grid(True, which="both")
 
 
 # Code below auto generates restructured text tables, copied from:
diff --git a/brian2/tests/features/speed.py b/brian2/tests/features/speed.py
index 0bdc3ca2..ef33e046 100644
--- a/brian2/tests/features/speed.py
+++ b/brian2/tests/features/speed.py
@@ -23,7 +23,17 @@ class LinearNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Linear 1D"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000, 1000000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        1000000,
+        10000000,
+        100000000,
+        261015625,
+    ]  # fail: 262031250
     n_label = "Num neurons"
 
     # configuration options
@@ -41,7 +51,16 @@ class HHNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Hodgkin-Huxley"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        1000000,
+        10000000,
+        102750000,
+    ]  # fail: 103125000
     n_label = "Num neurons"
 
     # configuration options
@@ -63,15 +82,15 @@ def run(self):
         # The model
         eqs = Equations(
             """
-            dv/dt = (gl*(El-v) - g_na*(m*m*m)*h*(v-ENa) - g_kd*(n*n*n*n)*(v-EK) + I)/Cm : volt
-            dm/dt = 0.32*(mV**-1)*(13.*mV-v+VT)/
-                (exp((13.*mV-v+VT)/(4.*mV))-1.)/ms*(1-m)-0.28*(mV**-1)*(v-VT-40.*mV)/
-                (exp((v-VT-40.*mV)/(5.*mV))-1.)/ms*m : 1
-            dn/dt = 0.032*(mV**-1)*(15.*mV-v+VT)/
-                (exp((15.*mV-v+VT)/(5.*mV))-1.)/ms*(1.-n)-.5*exp((10.*mV-v+VT)/(40.*mV))/ms*n : 1
-            dh/dt = 0.128*exp((17.*mV-v+VT)/(18.*mV))/ms*(1.-h)-4./(1+exp((40.*mV-v+VT)/(5.*mV)))/ms*h : 1
-            I : amp
-            """
+        dv/dt = (gl*(El-v) - g_na*(m*m*m)*h*(v-ENa) - g_kd*(n*n*n*n)*(v-EK) + I)/Cm : volt
+        dm/dt = 0.32*(mV**-1)*(13.*mV-v+VT)/
+            (exp((13.*mV-v+VT)/(4.*mV))-1.)/ms*(1-m)-0.28*(mV**-1)*(v-VT-40.*mV)/
+            (exp((v-VT-40.*mV)/(5.*mV))-1.)/ms*m : 1
+        dn/dt = 0.032*(mV**-1)*(15.*mV-v+VT)/
+            (exp((15.*mV-v+VT)/(5.*mV))-1.)/ms*(1.-n)-.5*exp((10.*mV-v+VT)/(40.*mV))/ms*n : 1
+        dh/dt = 0.128*exp((17.*mV-v+VT)/(18.*mV))/ms*(1.-h)-4./(1+exp((40.*mV-v+VT)/(5.*mV)))/ms*h : 1
+        I : amp
+        """
         )
         # Threshold and refractoriness are only used for spike counting
         group = NeuronGroup(
@@ -86,7 +105,7 @@ class CUBAFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "CUBA fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 3546875]  # fail: 3562500
     n_label = "Num neurons"
 
     # configuration options
@@ -130,7 +149,18 @@ class COBAHHFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "COBAHH fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]
+    n_range = [
+        100,
+        500,
+        1000,
+        5000,
+        10000,
+        50000,
+        100000,
+        500000,
+        1000000,
+        3781250,
+    ]  # fail: 3812500
     n_label = "Num neurons"
 
     # configuration options
@@ -160,24 +190,24 @@ def run(self):
         # The model
         eqs = Equations(
             """
-            dv/dt = (gl*(El-v)+ge*(Ee-v)+gi*(Ei-v)-
-                     g_na*(m*m*m)*h*(v-ENa)-
-                     g_kd*(n*n*n*n)*(v-EK))/Cm : volt
-            dm/dt = alpha_m*(1-m)-beta_m*m : 1
-            dn/dt = alpha_n*(1-n)-beta_n*n : 1
-            dh/dt = alpha_h*(1-h)-beta_h*h : 1
-            dge/dt = -ge*(1./taue) : siemens
-            dgi/dt = -gi*(1./taui) : siemens
-            alpha_m = 0.32*(mV**-1)*(13*mV-v+VT)/
-                     (exp((13*mV-v+VT)/(4*mV))-1.)/ms : Hz
-            beta_m = 0.28*(mV**-1)*(v-VT-40*mV)/
-                    (exp((v-VT-40*mV)/(5*mV))-1)/ms : Hz
-            alpha_h = 0.128*exp((17*mV-v+VT)/(18*mV))/ms : Hz
-            beta_h = 4./(1+exp((40*mV-v+VT)/(5*mV)))/ms : Hz
-            alpha_n = 0.032*(mV**-1)*(15*mV-v+VT)/
-                     (exp((15*mV-v+VT)/(5*mV))-1.)/ms : Hz
-            beta_n = .5*exp((10*mV-v+VT)/(40*mV))/ms : Hz
-            """
+        dv/dt = (gl*(El-v)+ge*(Ee-v)+gi*(Ei-v)-
+                 g_na*(m*m*m)*h*(v-ENa)-
+                 g_kd*(n*n*n*n)*(v-EK))/Cm : volt
+        dm/dt = alpha_m*(1-m)-beta_m*m : 1
+        dn/dt = alpha_n*(1-n)-beta_n*n : 1
+        dh/dt = alpha_h*(1-h)-beta_h*h : 1
+        dge/dt = -ge*(1./taue) : siemens
+        dgi/dt = -gi*(1./taui) : siemens
+        alpha_m = 0.32*(mV**-1)*(13*mV-v+VT)/
+                 (exp((13*mV-v+VT)/(4*mV))-1.)/ms : Hz
+        beta_m = 0.28*(mV**-1)*(v-VT-40*mV)/
+                (exp((v-VT-40*mV)/(5*mV))-1)/ms : Hz
+        alpha_h = 0.128*exp((17*mV-v+VT)/(18*mV))/ms : Hz
+        beta_h = 4./(1+exp((40*mV-v+VT)/(5*mV)))/ms : Hz
+        alpha_n = 0.032*(mV**-1)*(15*mV-v+VT)/
+                 (exp((15*mV-v+VT)/(5*mV))-1.)/ms : Hz
+        beta_n = .5*exp((10*mV-v+VT)/(40*mV))/ms : Hz
+        """
         )
 
         P = NeuronGroup(
@@ -242,16 +272,14 @@ def run(self):
         S = Synapses(
             poisson_input,
             neurons,
-            """
-            w : 1
-            dApre/dt = -Apre / taupre : 1 (event-driven)
-            dApost/dt = -Apost / taupost : 1 (event-driven)
-            """,
+            """w : 1
+                        dApre/dt = -Apre / taupre : 1 (event-driven)
+                        dApost/dt = -Apost / taupost : 1 (event-driven)""",
             on_pre="""ge += w
-                      Apre += dApre
-                      w = clip(w + Apost, 0, gmax)""",
+                            Apre += dApre
+                            w = clip(w + Apost, 0, gmax)""",
             on_post="""Apost += dApost
-                       w = clip(w + Apre, 0, gmax)""",
+                             w = clip(w + Apre, 0, gmax)""",
         )
         S.connect()
         S.w = "rand() * gmax"
@@ -263,7 +291,7 @@ def run(self):
 class SynapsesOnly:
     category = "Synapses only"
     tags = ["Synapses"]
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000]
     n_label = "Num neurons"
     duration = 1 * second
     # memory usage will be approximately p**2*rate*dt*N**2*bytes_per_synapse/1024**3 GB
@@ -291,7 +319,16 @@ class VerySparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Very sparse, medium rate (10s duration)"
     rate = 10 * Hz
     p = 0.02
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        500000,
+        1000000,
+        3875000,
+    ]  # fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -299,21 +336,47 @@ class SparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, medium rate (1s duration)"
     rate = 10 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        500000,
+        1000000,
+        1234375,
+    ]  # fail: 1242187  # weave max CPU time should be about 5m
 
 
 class DenseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Dense, medium rate (1s duration)"
     rate = 10 * Hz
     p = 1.0
-    n_range = [10, 100, 1000, 10000, 40000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        500000,
+        546875,
+    ]  # fail: 554687  # weave max CPU time should be about 4m
 
 
 class SparseLowRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, low rate (10s duration)"
     rate = 1 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        500000,
+        1000000,
+        3875000,
+    ]  # fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -321,7 +384,14 @@ class SparseHighRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, high rate (1s duration)"
     rate = 100 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000]
+    n_range = [
+        10,
+        100,
+        1000,
+        10000,
+        100000,
+        387500,
+    ]  # fail: 393750  # weave max CPU time should be about 5m
 
 
 if __name__ == "__main__":
